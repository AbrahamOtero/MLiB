{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU1X0Xs6zSNJXMcUmkuCLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamOtero/MLiB/blob/main/3_FeatureSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selection\n",
        "\n",
        "##Set up\n",
        "\n",
        "We import the libraries that we are going to need"
      ],
      "metadata": {
        "id": "CZxBwrxtVUsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lvRI0z-eVIky"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first import the iris data set:"
      ],
      "metadata": {
        "id": "LXggv4vzZ4Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/AbrahamOtero/MLiB/main/datasets/iris.csv'\n",
        "\n",
        "iris = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "tsdAD6yYZ3Y8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter feature selection methods\n",
        "\n",
        "To implement different filtering method strategies, **SelectKBest** can be used, which selects the number of attributes that we indicate in its constructor (through the parameter k) based on some score function. In this case, the chi-square function will be used. If the score_func used is **'mutual_info_classif'** it will use the information gain criterion. In the case where the class is metric, the **'f_regression'** criterion can be used."
      ],
      "metadata": {
        "id": "R2us__rky2RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# X will be the matrix with the features that we are going to evaluate and y the class\n",
        "X = iris.drop('class', axis=1)\n",
        "y = iris['class']\n",
        "\n",
        "# Apply SelectKBest with chi2 to select the 2 best attributes\n",
        "best_features = SelectKBest(score_func=chi2, k=2)\n",
        "fit = best_features.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = fit.get_support(indices=True)\n",
        "\n",
        "# Print the names of the selected attributes\n",
        "print(X.columns[feature_indices])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSkkfZDUaEdc",
        "outputId": "100af7be-28c4-48ab-a008-e645755b44f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['petal.length', 'petal.width'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the SlectKBest Estimator we can also see the scores obtained for each attribute:"
      ],
      "metadata": {
        "id": "xQqX501v0wGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the chi2 scores for each feature\n",
        "feature_scores = fit.scores_\n",
        "\n",
        "# Create a DataFrame to store the scores and feature names\n",
        "df_scores = pd.DataFrame({'Feature': X.columns, 'Chi2 Score': feature_scores})\n",
        "\n",
        "# Sort the DataFrame by Chi2 Score in descending order\n",
        "df_scores = df_scores.sort_values(by='Chi2 Score', ascending=False)\n",
        "\n",
        "# Print the sorted scores\n",
        "print(df_scores)\n"
      ],
      "metadata": {
        "id": "rwvvhgWs0G1s",
        "outputId": "aa4fc8ff-1c12-420e-ebd0-e0174e089ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Feature  Chi2 Score\n",
            "2  petal.length  116.312613\n",
            "3   petal.width   67.048360\n",
            "0  sepal.length   10.817821\n",
            "1   sepal.width    3.710728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see what scores we would get using information gain:"
      ],
      "metadata": {
        "id": "0A9Dnyjj2QwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Apply mutual_info_classif to get the scores\n",
        "feature_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Create a DataFrame to store the scores and feature names\n",
        "df_scores = pd.DataFrame({'Feature': X.columns, 'Mutual Info Score': feature_scores})\n",
        "\n",
        "# Sort the DataFrame by Mutual Info Score in descending order\n",
        "df_scores = df_scores.sort_values(by='Mutual Info Score', ascending=False)\n",
        "\n",
        "# Print the sorted scores\n",
        "print(df_scores)\n"
      ],
      "metadata": {
        "id": "h8WzRSWK12I2",
        "outputId": "0b97247e-938b-4e73-8b6e-afdd53750ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Feature  Mutual Info Score\n",
            "2  petal.length           0.987602\n",
            "3   petal.width           0.981737\n",
            "0  sepal.length           0.476764\n",
            "1   sepal.width           0.257040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper feature selection methods\n",
        "\n",
        "To carry out feature selection based on model wrappers, we can use the **RFE** (Recursive feature elimination)class, to which we must pass the model we want to use for the selection. In the example below, the model will be a decision tree."
      ],
      "metadata": {
        "id": "Mn8ewMaj2kKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# The model we will use will be a decision tree\n",
        "estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create RFE object to select 2 attributes based on decision tree\n",
        "selector = RFE(estimator, n_features_to_select=2)\n",
        "\n",
        "# Fitting the RFE object to the data\n",
        "selector = selector.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Print the names of the selected attributes\n",
        "print(X.columns[feature_indices])\n",
        "print(selector.support_)\n",
        "print(selector.ranking_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv_r3FGUalyc",
        "outputId": "698b9baa-6c59-4195-c3d3-9c5ca8a6da24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['petal.length', 'petal.width'], dtype='object')\n",
            "[False False  True  True]\n",
            "[2 3 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFE starts from all attributes and tries to eliminate them. If we want to use the opposite strategy (start from a set of attributes and add them) we can use **SequentialFeatureSelector**. The following example applies this strategy, also using a decision tree."
      ],
      "metadata": {
        "id": "6b3c7oud37sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "# The model we will use will be a decision tree\n",
        "estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create SFS object to select 2 attributes based on decision tree\n",
        "sfs = SequentialFeatureSelector(estimator, n_features_to_select=2)\n",
        "\n",
        "# Fitting the SFS object to the data\n",
        "sfs = sfs.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = sfs.get_support(indices=True)\n",
        "\n",
        "# Print the names of the selected attributes\n",
        "print(X.columns[feature_indices])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x9XwBSUabgM",
        "outputId": "9eadb07f-fd75-408a-cd52-3a55a075d4bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['petal.length', 'petal.width'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A full example\n",
        "\n",
        "Let's now look at a complete example where we will use several techniques to evaluate the attributes of the diabetes dataset. As you will see, as is often the case, the results are not completely consistent across techniques. Remember that these techniques are just heuristics."
      ],
      "metadata": {
        "id": "fG6T-hoV4yv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/AbrahamOtero/MLiB/main/datasets/diabetes.csv'\n",
        "\n",
        "diabetes = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "BpD1WOjU3PkW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X will be the matrix with the features that we are going to evaluate and y the class\n",
        "X = diabetes.drop('Outcome', axis=1)\n",
        "y = diabetes['Outcome']\n",
        "\n",
        "# Apply SelectKBest with mutual_info_classif to get the scores\n",
        "best_features = SelectKBest(score_func=mutual_info_classif, k=5)\n",
        "fit = best_features.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = fit.get_support(indices=True)\n",
        "print(X.columns[feature_indices])\n",
        "\n",
        "# Get the mutual_info_classif scores for each feature and print them\n",
        "feature_scores = fit.scores_\n",
        "df_scores = pd.DataFrame({'Feature': X.columns, 'Mutual Info Score': feature_scores})\n",
        "df_scores = df_scores.sort_values(by='Mutual Info Score', ascending=False)\n",
        "print(df_scores)\n",
        "\n",
        "# Now let's move to wrapper methods\n",
        "# The model we will use will be a decision tree\n",
        "estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create RFE object to select 4 attributes based on decision tree\n",
        "selector = RFE(estimator, n_features_to_select=4)\n",
        "\n",
        "# Fitting the RFE object to the data\n",
        "selector = selector.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = selector.get_support(indices=True)\n",
        "print(X.columns[feature_indices])\n",
        "\n",
        "# Create SequentialFeatureSelector object to select 4 attributes based on decision tree\n",
        "sfs = SequentialFeatureSelector(estimator, n_features_to_select=4)\n",
        "sfs = sfs.fit(X, y)\n",
        "\n",
        "# Get the indexes of the selected attributes\n",
        "feature_indices = sfs.get_support(indices=True)\n",
        "print(X.columns[feature_indices])\n"
      ],
      "metadata": {
        "id": "-7anEsr93feK",
        "outputId": "d5931375-e843-4e5f-c43f-16bc6b9497de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Glucose', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\n",
            "                    Feature  Mutual Info Score\n",
            "1                   Glucose           0.105345\n",
            "5                       BMI           0.082073\n",
            "7                       Age           0.050209\n",
            "4                   Insulin           0.046151\n",
            "6  DiabetesPedigreeFunction           0.014338\n",
            "3             SkinThickness           0.012589\n",
            "0               Pregnancies           0.005137\n",
            "2             BloodPressure           0.000000\n",
            "Index(['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\n",
            "Index(['Glucose', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, it is clear that Glucose is the most relevant attribute, as it has been consistently selected by all methods. Insulin has not been selected by either of the two wrapping methods, nor evaluated very well by the Information Gain method. BMI and Age have been selected by one of the wrapping methods, and have been evaluated well by the filter method. For the rest of the attributes, the value of their values ​​is not so clear."
      ],
      "metadata": {
        "id": "YE7uKgde5_2X"
      }
    }
  ]
}