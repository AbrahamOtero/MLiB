{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqukO3DzEcVk/NDaijQQH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamOtero/MLiB/blob/main/6_MetaModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combination of models\n",
        "We import the libraries that we are going to need:"
      ],
      "metadata": {
        "id": "X7T4z_H9CERP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hvg5dtqxsw0d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting\n",
        "\n",
        "We will start by implementing a voting strategy using **VotingClassifier**. We will use three models (although you can add more models if you want): a decision tree, K nearest neighbors, and Naive Bayes. We will apply it to the diabetes dataset, and perform a 10-fold evaluation."
      ],
      "metadata": {
        "id": "DLLLQbL2t9LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/AbrahamOtero/MLiB/main/datasets/diabetes.csv'\n",
        "\n",
        "diabetes = pd.read_csv(url)\n",
        "\n",
        "# The featrures\n",
        "X = diabetes.iloc[:, :-1]\n",
        "# The class\n",
        "y = diabetes.iloc[:,-1]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# We will reserve a set of test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# We build the voting-based classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    # Map of classifiers that \"vote\". We indicate their name (any string of characters) and the classifier\n",
        "    estimators=[\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('knn', KNeighborsClassifier()),\n",
        "        ('gnb', GaussianNB())\n",
        "    ]\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "scores = cross_val_score(voting_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "# Calculate and display the average accuracy value and standard deviation for the scores of each fold\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxtGJ4-ctAG1",
        "outputId": "755c9fa9-0841-49a6-8c8b-68e712e44c75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.7543478260869565\n",
            "Standard deviation: 0.05149008383620441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how each of the three classifiers performs, as well as the performance of the voting model composed of the three classifiers. We will use the 10% of data that we saved for testing:"
      ],
      "metadata": {
        "id": "0e702xQuFFdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in voting_clf.named_estimators_.items():\n",
        "    print(name, \"=\", clf.score(X_test, y_test))\n",
        "\n",
        "print(\"Voting: \", voting_clf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvA2Rah4t0GW",
        "outputId": "fdea29c7-fd68-4b34-8886-d52a123005c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt = 0.7175324675324676\n",
            "knn = 0.7142857142857143\n",
            "gnb = 0.7597402597402597\n",
            "Voting:  0.775974025974026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the performance of the model composed of the three classifiers is slightly higher than the performance of each of them. In practice this does not always have to be the case (especially if the errors of the different classifiers are correlated, or if one classifier is much better than the others). But we can often improve the performance of each individual model using voting.\n",
        "\n"
      ],
      "metadata": {
        "id": "dAPz1YBdFHY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging\n",
        "\n",
        "An obvious approach to the \"vote\" strategy is to use multiple versions of the same classifier over altered versions (using sampling) of the data set. For example, using multiple decision trees trained over different versions of a datset. We can do this easily by using **BaggingClassifier**:"
      ],
      "metadata": {
        "id": "cil0QXvTt_sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# To build each model we will use a number of samples equal to half of those available.\n",
        "# This is a hyperparameter you may change\n",
        "n_smples = round(X_train.shape[1]/2)\n",
        "\n",
        "#We created a Bagging classifier composed of 100 decision trees\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=n_smples)\n",
        "socres = cross_val_score(bag_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "# Calculate and display the average accuracy value and standard deviation for the scores of each fold\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHOLwXdRuDT0",
        "outputId": "2ce52c95-5cf7-4d44-aee0-1d17c592d9c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.7543478260869565\n",
            "Standard deviation: 0.05149008383620441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests\n",
        "\n",
        "A Random Forest is similar to a bag of decision trees. The main difference is that the decision trees are going to be built with different parameters (for example, maximum number of depth levels, a minimum number of instances in each different leaf). When using Bagging all the classifiers have been built with the same parameters, but training over different dataset created using sampling.\n"
      ],
      "metadata": {
        "id": "vYKnhsD_uRY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "scores = cross_val_score(rnd_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "# Calculate and display the average accuracy value and standard deviation for the scores of each fold\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3C92yBOuS5T",
        "outputId": "9f38473d-4dc3-4c0d-9d1c-073d1fed71e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.7565217391304349\n",
            "Standard deviation: 0.04742918310711181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An interesting property of the Scikit Learn random tree classifier is that it can evaluate how important each attribute is based on how many times it was chosen to be used by the decision trees. To do this we will have to re-fit the classifier on the data (as we used cross_val_score in the previous code):"
      ],
      "metadata": {
        "id": "HKgCQGpZvCF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the classifier\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "# Feature relevance\n",
        "print ('Relevance Feature')\n",
        "for score, name in zip(rnd_clf.feature_importances_, diabetes.columns):\n",
        "    print(round(score, 2), '\\t' , name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq-qIXS5ukvI",
        "outputId": "1cd9c124-c885-4141-f949-309d23dacef4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevance Feature\n",
            "0.08 \t Pregnancies\n",
            "0.25 \t Glucose\n",
            "0.09 \t BloodPressure\n",
            "0.07 \t SkinThickness\n",
            "0.07 \t Insulin\n",
            "0.16 \t BMI\n",
            "0.12 \t DiabetesPedigreeFunction\n",
            "0.15 \t Age\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "To use Boosting we can use **AdaBoostClassifier**. In each iteration, this classifier places more emphasis on the errors made by the previous classifiers."
      ],
      "metadata": {
        "id": "e5XjWS2avQQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# algorithm= 'SAMME'avoids warning due to use of a deprecated strategy\n",
        "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=100, algorithm= 'SAMME')\n",
        "\n",
        "scores = cross_val_score(ada_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "# Calculate and display the average accuracy value and standard deviation for the scores of each fold\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GbKADBIvH6U",
        "outputId": "4ebc500d-af39-450d-91d3-335cf8795a01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.717391304347826\n",
            "Standard deviation: 0.04861017342390848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking\n",
        "\n",
        "Finall we will use Stacking using the classifier **StackingClassifier**. It will have 3 level 0 models (decision trees, nearest neighbors and Bayesian classifier) ​​and the level 1 classifier will be a neural network."
      ],
      "metadata": {
        "id": "Xbfoxezov5Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "     # Map of level 0 models\n",
        "    estimators=[\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('knn', KNeighborsClassifier()),\n",
        "        ('gnb', GaussianNB())\n",
        "    ],\n",
        "     # Level 1 model than takes as inputs the predictions of the previous models\n",
        "    final_estimator=MLPClassifier(hidden_layer_sizes=(10,10))\n",
        ")\n",
        "\n",
        "scores = cross_val_score(stacking_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "# Calculate and display the average accuracy value and standard deviation for the scores of each fold\n",
        "print(\"Mean accuracy:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKQirudpv680",
        "outputId": "c61b8eb7-173e-4c2f-fbac-fb8ebfcc8fbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.7413043478260869\n",
            "Standard deviation: 0.053647663822839434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of classifiers based on cost\n",
        "\n",
        "Unfortunately Scikit learn does not have functionality directly to evaluate classifiers based on cost. We will rely on the following code (obtained from the following repository https://github.com/Treers/MetaCost). You will need to run this to create the cost based classifier. If you use it in your project, you will also need to load this class in the environment."
      ],
      "metadata": {
        "id": "nuiR6dIQZuGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run this code to loas the MetaCost classifier in the environment\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "\n",
        "class MetaCost(object):\n",
        "\n",
        "    \"\"\"A procedure for making error-based classifiers cost-sensitive\n",
        "\n",
        "    >>> from sklearn.datasets import load_iris\n",
        "    >>> from sklearn.linear_model import LogisticRegression\n",
        "    >>> import pandas as pd\n",
        "    >>> import numpy as np\n",
        "    >>> S = pd.DataFrame(load_iris().data)\n",
        "    >>> S['target'] = load_iris().target\n",
        "    >>> LR = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
        "    >>> C = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n",
        "    >>> model = MetaCost(S, LR, C).fit('target', 3)\n",
        "    >>> model.predict_proba(load_iris().data[[2]])\n",
        "    >>> model.score(S[[0, 1, 2, 3]].values, S['target'])\n",
        "\n",
        "    .. note:: The form of the cost matrix C must be as follows:\n",
        "    +---------------+----------+----------+----------+\n",
        "    |  actual class |          |          |          |\n",
        "    +               |          |          |          |\n",
        "    |   +           | y(x)=j_1 | y(x)=j_2 | y(x)=j_3 |\n",
        "    |       +       |          |          |          |\n",
        "    |           +   |          |          |          |\n",
        "    |predicted class|          |          |          |\n",
        "    +---------------+----------+----------+----------+\n",
        "    |   h(x)=j_1    |    0     |    a     |     b    |\n",
        "    |   h(x)=j_2    |    c     |    0     |     d    |\n",
        "    |   h(x)=j_3    |    e     |    f     |     0    |\n",
        "    +---------------+----------+----------+----------+\n",
        "    | C = np.array([[0, a, b],[c, 0 , d],[e, f, 0]]) |\n",
        "    +------------------------------------------------+\n",
        "    \"\"\"\n",
        "    def __init__(self, S, L, C, m=50, n=1, p=True, q=True):\n",
        "        \"\"\"\n",
        "        :param S: The training set\n",
        "        :param L: A classification learning algorithm\n",
        "        :param C: A cost matrix\n",
        "        :param q: Is True iff all resamples are to be used  for each examples\n",
        "        :param m: The number of resamples to generate\n",
        "        :param n: The number of examples in each resample\n",
        "        :param p: Is True iff L produces class probabilities\n",
        "        \"\"\"\n",
        "        if not isinstance(S, pd.DataFrame):\n",
        "            raise ValueError('S must be a DataFrame object')\n",
        "        new_index = list(range(len(S)))\n",
        "        S.index = new_index\n",
        "        self.S = S\n",
        "        self.L = L\n",
        "        self.C = C\n",
        "        self.m = m\n",
        "        self.n = len(S) * n\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "\n",
        "    def fit(self, flag, num_class):\n",
        "        \"\"\"\n",
        "        :param flag: The name of classification labels\n",
        "        :param num_class: The number of classes\n",
        "        :return: Classifier\n",
        "        \"\"\"\n",
        "        col = [col for col in self.S.columns if col != flag]\n",
        "        S_ = {}\n",
        "        M = []\n",
        "\n",
        "        for i in range(self.m):\n",
        "            # Let S_[i] be a resample of S with self.n examples\n",
        "            S_[i] = self.S.sample(n=self.n, replace=True)\n",
        "\n",
        "            X = S_[i][col].values\n",
        "            y = S_[i][flag].values\n",
        "\n",
        "            # Let M[i] = model produced by applying L to S_[i]\n",
        "            model = clone(self.L)\n",
        "            M.append(model.fit(X, y))\n",
        "\n",
        "        label = []\n",
        "        S_array = self.S[col].values\n",
        "        for i in range(len(self.S)):\n",
        "            if not self.q:\n",
        "                k_th = [k for k, v in S_.items() if i not in v.index]\n",
        "                M_ = list(np.array(M)[k_th])\n",
        "            else:\n",
        "                M_ = M\n",
        "\n",
        "            if self.p:\n",
        "                P_j = [model.predict_proba(S_array[[i]]) for model in M_]\n",
        "            else:\n",
        "                P_j = []\n",
        "                vector = [0] * num_class\n",
        "                for model in M_:\n",
        "                    vector[model.predict(S_array[[i]])] = 1\n",
        "                    P_j.append(vector)\n",
        "\n",
        "            # Calculate P(j|x)\n",
        "            P = np.array(np.mean(P_j, 0)).T\n",
        "\n",
        "            # Relabel\n",
        "            label.append(np.argmin(self.C.dot(P)))\n",
        "\n",
        "        # Model produced by applying L to S with relabeled y\n",
        "        X_train = self.S[col].values\n",
        "        y_train = np.array(label)\n",
        "        model_new = clone(self.L)\n",
        "        model_new.fit(X_train, y_train)\n",
        "\n",
        "        return model_new"
      ],
      "metadata": {
        "id": "8rNWkbHeSCaJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by training a decision tree without considering the cost using the diabees data set. We will show the accuracy and the confusion matrix:"
      ],
      "metadata": {
        "id": "cc6381C7kZ-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Assuming X and y are already defined (from your previous code)\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_mat_tree = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat_tree)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat_tree,\n",
        "                              display_labels=dt_classifier.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "lJFybkI_kZPu",
        "outputId": "2b176f78-56db-4e51-b3cf-e71967bf275d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7305194805194806\n",
            "Confusion Matrix:\n",
            "[[152  54]\n",
            " [ 29  73]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the confusion matrix generated by SKlearn has the true classes in the rows, and the predicted classes in the columns. However, if you look at the **MetaCost documentation, it expects just the opposite**: true classes in the columns and predicted classes in the rows. We will need to take this into account when interpreting the results and calculating the costs.\n",
        "\n",
        "Now we shall train MetaCost, but with a cost matrix where all errors have the same cost."
      ],
      "metadata": {
        "id": "rSsp_xq3lvFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost matrix\n",
        "cost_matrix = np.array([[0, 1], [1, 0]])\n",
        "\n",
        "# Combine X_train and y_train into a single DataFrame\n",
        "# MetaCost needs all the data to be arranged in a DataFrame\n",
        "train_df = pd.DataFrame(data=X_train)\n",
        "train_df['Outcome'] = pd.array(y_train)\n",
        "\n",
        "# Create a MetaCost object with a DecisionTreeClassifier\n",
        "metacost_model = MetaCost(train_df, DecisionTreeClassifier(random_state=42), cost_matrix)\n",
        "\n",
        "# Fit the model to the training data.\n",
        "# We need to indicate the colum of the class ('Outcome') and the number of classes (2)\n",
        "model = metacost_model.fit('Outcome', 2)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUUCouNikq4e",
        "outputId": "5f53b87a-ea66-445d-fb39-3f9966e10e90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7305194805194806\n",
            "Confusion Matrix:\n",
            "[[152  54]\n",
            " [ 29  73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how by using the same cost for all errors we have obtained exactly the same confusion matrix as in the previous case (the standard classifier).\n",
        "\n",
        "Now we will carry out the training considering that classifying a diabetic patient as healthy has a cost 10 times higher than classifying a healthy patient as diabetic.\n",
        "\n",
        "(Note: The MetaCost classifier does not provide an interface that permits an easy control of its random state. If you run code that uses this classifier, you may get slightly different results than the text indicates for this reason.)"
      ],
      "metadata": {
        "id": "xSALARA-mB5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost matrix\n",
        "cost_matrix = np.array([[0, 10], [1, 0]])\n",
        "\n",
        "# Create a MetaCost object with a DecisionTreeClassifier\n",
        "metacost_model = MetaCost(train_df, DecisionTreeClassifier(random_state=42), cost_matrix)\n",
        "\n",
        "# Fit the model to the training data.\n",
        "# We need to indicate the colum of the class ('Outcome') and the number of classes (2)\n",
        "model = metacost_model.fit('Outcome', 2)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calculate the cost of the classifier\n",
        "# We have to transpose the cost matrix because the confusion matrix arranges\n",
        "# the rows and columns in the opposite way\n",
        "cost = np.sum(confusion_mat * np.transpose(cost_matrix))\n",
        "\n",
        "# Print the cost\n",
        "print(\"Cost:\", cost)\n",
        "# Print the cost per instance\n",
        "print(\"Cost per instance:\", cost/len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4-r9IWamBRs",
        "outputId": "3b9f7011-0d55-4d61-9d3b-5404be38e13a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6201298701298701\n",
            "Confusion Matrix:\n",
            "[[100 106]\n",
            " [ 11  91]]\n",
            "Cost: 216\n",
            "Cost per instance: 0.7012987012987013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier's performance has dropped from about 73% to 67%. Its cost is 216 (0.70 per instance). What was the cost of the classifier that had an accuracy of 73%?"
      ],
      "metadata": {
        "id": "sBRZXZuF3AtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_mat_tree)\n",
        "\n",
        "# Calculate the cost of the classifier\n",
        "cost = np.sum(confusion_mat_tree * np.transpose(cost_matrix))\n",
        "\n",
        "# Print the cost\n",
        "print(\"Cost:\", cost)\n",
        "# Print the cost per instance\n",
        "print(\"Cost per instance:\", cost/len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9Sat5A5miS9",
        "outputId": "886464ac-175e-4841-b5f1-a8877213f37b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[152  54]\n",
            " [ 29  73]]\n",
            "Cost: 344\n",
            "Cost per instance: 1.1168831168831168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost of the classifier that had an accuracy of 73% is 344  (1.12 per instance), compared to the cost of 209. The classifier trained with MetaCost has lower accuracy, but lower cost. The classifier trained in the normal way has higher accuracy, but also higher cost. It makes fewer errors, but the type of errors it makes are the ones we want to avoid (they have higher cost).\n",
        "\n",
        "Next, we will perform a cost-based training and evaluation using a 10-fold validation. MetaCost is not a SKlearn Predictor (it does not implement all the methods it should) so we will have to generate the folds manually."
      ],
      "metadata": {
        "id": "aPC171h31h8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# The featrures\n",
        "X = diabetes.iloc[:, :-1]\n",
        "# The class\n",
        "y = diabetes.iloc[:,-1]\n",
        "\n",
        "# Define the number of folds\n",
        "n_splits = 10\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the  classifier\n",
        "classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the cost matrix\n",
        "cost_matrix = np.array([[0, 10], [1, 0]])\n",
        "\n",
        "# Create an empty list to store the accuracy and cost scores for each fold\n",
        "accuracy_scores = []\n",
        "cost_scores = []\n",
        "\n",
        "# Iterate through the folds\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  # Split the data into training and validation sets for this fold\n",
        "  X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  # Create a DataFrame for MetaCost\n",
        "  train_df_fold = pd.DataFrame(data=X_train_fold)\n",
        "  train_df_fold['Outcome'] = pd.array(y_train_fold)\n",
        "\n",
        "  # Create and fit the MetaCost model for this fold\n",
        "  metacost_model = MetaCost(train_df_fold, classifier, cost_matrix)\n",
        "  model = metacost_model.fit('Outcome', 2)\n",
        "\n",
        "  # Make predictions on the validation set for this fold\n",
        "  y_pred_fold = model.predict(X_val_fold)\n",
        "\n",
        "  # Calculate the accuracy for this fold\n",
        "  accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)\n",
        "  accuracy_scores.append(accuracy_fold)\n",
        "\n",
        "  # Calculate the cost for this fold\n",
        "  confusion_mat = confusion_matrix(y_val_fold, y_pred_fold)\n",
        "  cost = np.sum(confusion_mat * np.transpose(cost_matrix))\n",
        "  cost_scores.append(cost)\n",
        "\n",
        "#End of the cross fold validation loop\n",
        "\n",
        "# Calculate and display the average accuracy and cost across all folds\n",
        "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
        "print(\"Standard deviation:\", np.std(accuracy_scores))\n",
        "print(\"Mean cost:\", np.mean(cost_scores))\n",
        "print(\"Standard deviation:\", np.std(cost_scores))\n",
        "divisor = len (y_val_fold)\n",
        "cost_scores_per_instance = [x / divisor for x in cost_scores]\n",
        "print(\"Mean cost:\", np.mean(cost_scores_per_instance))\n",
        "print(\"Standard deviation:\", np.std(cost_scores_per_instance))"
      ],
      "metadata": {
        "id": "oHJGwcnv9DtE",
        "outputId": "c494974d-3ce9-455d-a729-7f24e7f5c34f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.6573137388926863\n",
            "Standard deviation: 0.06315953140772021\n",
            "Mean cost: 63.2\n",
            "Standard deviation: 23.51510153071851\n",
            "Mean cost: 0.8315789473684211\n",
            "Standard deviation: 0.30940923066734877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that when comparing these results to the previous ones the total cost is not comparable, as the size of the data sets when using 10-fold validation is quite different. You should make the comparison at the cost per instance level. The cost is 0.83 per instance.\n",
        "\n",
        "Let's compare these results to performing a 10-fold validation directly on the classifier, without using cost-based training (although we will also calculate the cost metric for this classifier, which has not taken cost into account during its training):"
      ],
      "metadata": {
        "id": "wBBd2ZEl-9vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the cost matrix; it will only be used to evalutate; not to train\n",
        "cost_matrix = np.array([[0, 10], [1, 0]])\n",
        "\n",
        "# Create an empty list to store the accuracy and cost scores for each fold\n",
        "accuracy_scores = []\n",
        "cost_scores = []\n",
        "\n",
        "# Iterate through the folds\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "  # Split the data into training and validation sets for this fold\n",
        "  X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  # Create and fit the model for this fold\n",
        "  model = DecisionTreeClassifier(random_state=42)\n",
        "  model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "  # Make predictions on the validation set for this fold\n",
        "  y_pred_fold = model.predict(X_val_fold)\n",
        "\n",
        "  # Calculate the accuracy for this fold\n",
        "  accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)\n",
        "  accuracy_scores.append(accuracy_fold)\n",
        "\n",
        "  # Calculate the cost for this fold\n",
        "  confusion_mat = confusion_matrix(y_val_fold, y_pred_fold)\n",
        "  cost = np.sum(confusion_mat * np.transpose(cost_matrix))\n",
        "  cost_scores.append(cost)\n",
        "\n",
        "#End of the cross fold validation loop\n",
        "\n",
        "# Calculate and display the average accuracy and cost across all folds\n",
        "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
        "print(\"Standard deviation:\", np.std(accuracy_scores))\n",
        "print(\"Mean cost:\", np.mean(cost_scores))\n",
        "print(\"Standard deviation:\", np.std(cost_scores))\n",
        "divisor = len (y_val_fold)\n",
        "cost_scores_per_instance = [x / divisor for x in cost_scores]\n",
        "print(\"Mean cost:\", np.mean(cost_scores_per_instance))\n",
        "print(\"Standard deviation:\", np.std(cost_scores_per_instance))"
      ],
      "metadata": {
        "id": "nim6OWEUBulh",
        "outputId": "48412a44-39bc-49e7-8611-0e4bcdea1589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.702939166097061\n",
            "Standard deviation: 0.060908555498988284\n",
            "Mean cost: 125.4\n",
            "Standard deviation: 28.281442678901655\n",
            "Mean cost: 1.65\n",
            "Standard deviation: 0.37212424577502173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average cost per instance is now 1.65+-0.37, compared to 0.83+-0.31 when we performed a cost-based evaluation. However, the accuracy was 0.66 +- 0.06, while in cost-based training the accuracy was 0.62 +-0.06. As always, we have had to sacrifice some accuracy to decrease the cost.\n",
        "\n"
      ],
      "metadata": {
        "id": "yXLKThDNBpox"
      }
    }
  ]
}